{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intall all necessary dependencies ###\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### get all necessary imports ###\n",
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.preprocessing import scale\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(return_indices=True)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "#create a list of english stopwords\n",
    "#add words with no info used in twitter\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.add(\"http\")\n",
    "stop_words.add(\"https\")\n",
    "stop_words.add(\"co\")\n",
    "stop_words.add(\"amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (625, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|███████████████████████████████████████████████████████████████| 625/625 [00:00<00:00, 12288.25it/s]\n",
      "0it [00:00, ?it/s]C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "500it [00:00, 100313.40it/s]\n",
      "125it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "500it [00:00, 6184.81it/s]\n",
      "125it [00:00, 3686.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 1s - loss: 0.7255 - acc: 0.4600\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.5499 - acc: 0.5000\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.5119 - acc: 0.5060\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.4973 - acc: 0.5100\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.4893 - acc: 0.5120\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.4833 - acc: 0.5180\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.4786 - acc: 0.5240\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.4751 - acc: 0.5280\n",
      "ACC:  0.5840000009536743\n"
     ]
    }
   ],
   "source": [
    "### get the data from the excel file ###\n",
    "def ingest():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "    \n",
    "    ### ### ### USE THIS FOR SENTIMENT CAN BE 1 POINT AWAY ### ### ###\n",
    "    ### ### ### LARGER SAMPLE SIZE ### ### ###\n",
    "    \n",
    "    #data = data[data['standard-dev'] != 1]\n",
    "    \n",
    "    ### ### ### USE THIS FOR SENTIMENT MUST BE EQuAL ### ### ###\n",
    "    ### ### ### SMALLER SAMPLE SIZE ### ### ###\n",
    "    \n",
    "    data = data[data['standard-dev'] == 0]\n",
    "    \n",
    "    ### #### ### ### ###\n",
    "    data.drop(['time' , 'sent-1' , 'sent-2' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    return data\n",
    "\n",
    "def ingest_sent_one():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "        \n",
    "    data.drop(['time' , 'average' , 'sent-2' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    data.columns = [ 'average' , 'text']\n",
    "    return data\n",
    "\n",
    "def ingest_sent_two():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "    \n",
    "    data.drop(['time' , 'sent-1' , 'average' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    data.columns = [ 'average' , 'text']\n",
    "    return data\n",
    "\n",
    "data = ingest()\n",
    "#data = ingest_sent_one() #### USE THIS TO TEST SENT ONE\n",
    "#data = ingest_sent_two() ### USE THIS TO TEST SENT TWO\n",
    "\n",
    "### function to tokenize the tweets and remove stop words ###\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        new_tokens = list()\n",
    "        for w in tokens:\n",
    "            if (not w in stop_words):\n",
    "                new_tokens.append(w)\n",
    "        return new_tokens\n",
    "    except:\n",
    "        return 'NC'\n",
    "    \n",
    "### this functions actually runs the tokenize function above ###\n",
    "def postprocess(data, n=1000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['text'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)\n",
    "\n",
    "### separate the data to test and train ###\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.tokens),\n",
    "                                                    np.array(data.average), test_size=0.2)\n",
    "\n",
    "### give the tweets a label\n",
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')\n",
    "\n",
    "### vectorize the words in tweets###\n",
    "n_dim = 200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=1)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)] , total_words=tweet_w2v.corpus_count , epochs=9)\n",
    "\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))\n",
    "\n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=n_dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=8, batch_size=10, verbose=2)\n",
    "\n",
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=10, verbose=2)\n",
    "print('ACC: ' , score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (625, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|████████████████████████████████████████████████████████████████| 625/625 [00:00<00:00, 9641.01it/s]\n",
      "0it [00:00, ?it/s]C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "500it [00:00, 99707.70it/s]\n",
      "125it [00:00, 125517.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 83548.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "500it [00:00, 5628.85it/s]\n",
      "125it [00:00, 4636.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 1s - loss: 0.7332 - acc: 0.3695\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.6159 - acc: 0.4121\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.5674 - acc: 0.4587\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.5461 - acc: 0.4703\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.5214 - acc: 0.5155\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.5022 - acc: 0.5439\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.4893 - acc: 0.5401\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.4738 - acc: 0.5478\n",
      "ACC:  0.5040000092983246\n"
     ]
    }
   ],
   "source": [
    "### get the data from the excel file ###\n",
    "def ingest():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "    \n",
    "    ### ### ### USE THIS FOR SENTIMENT CAN BE 1 POINT AWAY ### ### ###\n",
    "    ### ### ### LARGER SAMPLE SIZE ### ### ###\n",
    "    \n",
    "    #data = data[data['standard-dev'] != 1]\n",
    "    \n",
    "    ### ### ### USE THIS FOR SENTIMENT MUST BE EQuAL ### ### ###\n",
    "    ### ### ### SMALLER SAMPLE SIZE ### ### ###\n",
    "    \n",
    "    data = data[data['standard-dev'] == 0]\n",
    "    \n",
    "    ### #### ### ### ###\n",
    "    data.drop(['time' , 'sent-1' , 'sent-2' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    return data\n",
    "\n",
    "def ingest_sent_one():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "        \n",
    "    data.drop(['time' , 'average' , 'sent-2' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    data.columns = [ 'average' , 'text']\n",
    "    return data\n",
    "\n",
    "def ingest_sent_two():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "    \n",
    "    data.drop(['time' , 'sent-1' , 'average' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    data.columns = [ 'average' , 'text']\n",
    "    return data\n",
    "\n",
    "data = ingest()\n",
    "#data = ingest_sent_one() #### USE THIS TO TEST SENT ONE\n",
    "#data = ingest_sent_two() ### USE THIS TO TEST SENT TWO\n",
    "\n",
    "### function to tokenize the tweets and remove stop words ###\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        new_tokens = list()\n",
    "        for w in tokens:\n",
    "            if (not w in stop_words):\n",
    "                new_tokens.append(w)\n",
    "        return new_tokens\n",
    "    except:\n",
    "        return 'NC'\n",
    "    \n",
    "### this functions actually runs the tokenize function above ###\n",
    "def postprocess(data, n=1000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['text'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)\n",
    "\n",
    "### separate the data to test and train ###\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.tokens),\n",
    "                                                    np.array(data.average), test_size=0.2)\n",
    "\n",
    "### give the tweets a label\n",
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')\n",
    "\n",
    "### vectorize the words in tweets###\n",
    "n_dim = 200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=1)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)] , total_words=tweet_w2v.corpus_count , epochs=9)\n",
    "\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))\n",
    "\n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=n_dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### OVERSAMPLE ###\n",
    "\n",
    "X = train_vecs_w2v\n",
    "y = y_train\n",
    "\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "\n",
    "### OVERSAMPLE ###\n",
    "\n",
    "model.fit(X_ros, y_ros, epochs=8, batch_size=10, verbose=2)\n",
    "\n",
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=10, verbose=2)\n",
    "print('ACC: ' , score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (959, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|███████████████████████████████████████████████████████████████| 959/959 [00:00<00:00, 10869.89it/s]\n",
      "0it [00:00, ?it/s]C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "767it [00:00, 109852.52it/s]\n",
      "192it [00:00, 96098.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 767/767 [00:00<00:00, 769256.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 767/767 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "767it [00:00, 4836.63it/s]\n",
      "192it [00:00, 3632.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 1s - loss: 0.7851 - acc: 0.3413\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.6688 - acc: 0.3611\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.6313 - acc: 0.3611\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.6121 - acc: 0.3889\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.5883 - acc: 0.3968\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.5768 - acc: 0.4087\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.5632 - acc: 0.4286\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.5402 - acc: 0.4365\n",
      "ACC:  0.5572916710128387\n"
     ]
    }
   ],
   "source": [
    "### get the data from the excel file ###\n",
    "def ingest():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "    \n",
    "    ### ### ### USE THIS FOR SENTIMENT CAN BE 1 POINT AWAY ### ### ###\n",
    "    ### ### ### LARGER SAMPLE SIZE ### ### ###\n",
    "    \n",
    "    #data = data[data['standard-dev'] != 1]\n",
    "    \n",
    "    ### ### ### USE THIS FOR SENTIMENT MUST BE EQuAL ### ### ###\n",
    "    ### ### ### SMALLER SAMPLE SIZE ### ### ###\n",
    "    \n",
    "    data = data[data['standard-dev'] == 0]\n",
    "    \n",
    "    ### #### ### ### ###\n",
    "    data.drop(['time' , 'sent-1' , 'sent-2' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    return data\n",
    "\n",
    "def ingest_sent_one():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "        \n",
    "    data.drop(['time' , 'average' , 'sent-2' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    data.columns = [ 'average' , 'text']\n",
    "    return data\n",
    "\n",
    "def ingest_sent_two():\n",
    "    data = pd.read_excel('twitter_data_w_sent.xlsx', sheet_name='data')\n",
    "    \n",
    "    data.drop(['time' , 'sent-1' , 'average' , 'standard-dev'] , axis=1 , inplace=True)\n",
    "    print('dataset loaded with shape', data.shape)  \n",
    "    data.columns = [ 'average' , 'text']\n",
    "    return data\n",
    "\n",
    "#data = ingest()\n",
    "#data = ingest_sent_one() #### USE THIS TO TEST SENT ONE\n",
    "data = ingest_sent_two() ### USE THIS TO TEST SENT TWO\n",
    "\n",
    "### function to tokenize the tweets and remove stop words ###\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        new_tokens = list()\n",
    "        for w in tokens:\n",
    "            if (not w in stop_words):\n",
    "                new_tokens.append(w)\n",
    "        return new_tokens\n",
    "    except:\n",
    "        return 'NC'\n",
    "    \n",
    "### this functions actually runs the tokenize function above ###\n",
    "def postprocess(data, n=1000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['text'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)\n",
    "\n",
    "### separate the data to test and train ###\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.tokens),\n",
    "                                                    np.array(data.average), test_size=0.2)\n",
    "\n",
    "### give the tweets a label\n",
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')\n",
    "\n",
    "### vectorize the words in tweets###\n",
    "n_dim = 200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=1)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)] , total_words=tweet_w2v.corpus_count , epochs=9)\n",
    "\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))\n",
    "\n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=n_dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "### OVERSAMPLE ###\n",
    "\n",
    "X = train_vecs_w2v\n",
    "y = y_train\n",
    "\n",
    "X_rus, y_rus, id_rus = rus.fit_sample(X, y)\n",
    "\n",
    "### OVERSAMPLE ###\n",
    "\n",
    "model.fit(X_rus, y_rus, epochs=8, batch_size=10, verbose=2)\n",
    "\n",
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=10, verbose=2)\n",
    "print('ACC: ' , score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
