{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in\n"
     ]
    }
   ],
   "source": [
    "#tweepy package to coolect tweets\n",
    "import tweepy\n",
    "import twitter_keys\n",
    "\n",
    "#authentication\n",
    "auth = tweepy.OAuthHandler(twitter_keys.CONSUMER_KEY, twitter_keys.CONSUMER_SECRET)\n",
    "auth.set_access_token(twitter_keys.ACCESS_KEY, twitter_keys.ACCESS_SECRET)\n",
    "\n",
    "#create instance of api\n",
    "api = tweepy.API(auth , wait_on_rate_limit=True)\n",
    "print(\"logged in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done querying.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py:832: UserWarning: Ignoring URL 'https://t.co/fqgZAw7NBB\n",
      "For%20those%20in%20our%20government%20who%20want%20to%20let%20the%20illegal%20caravan%20inside%20our%20borders%20without%20vetting,%20LISTEN%20TO%20THIS!!!!%20Our%20men%20DIED%20for%20your%20freedom.%20DON'T%20make%20their%20deaths%20in%20vain!' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n",
      "C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:229: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:230: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\nat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full file created:  data/2018-11-20/caravan/caravan_12-23_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "def query_twitter(q = \"twitter\"):\n",
    "    #ENTER SEARCH TERM HERE\n",
    "    search = q\n",
    "\n",
    "    #enter the max number of tweets\n",
    "    max_tweets = 5000\n",
    "\n",
    "    #arrays to collect data of only non rt tweets\n",
    "    timestamps = list()\n",
    "    tweets = list()\n",
    "    tweets_retweets = list()\n",
    "    tweets_likes = list()\n",
    "    tweets_length = list()\n",
    "    tweets_source = list()\n",
    "    tweets_sentiment = list()\n",
    "    tweets_raw_sentiment = list()\n",
    "\n",
    "    #arrays to collect data of all tweets\n",
    "    all_tweets_timestamps = list();\n",
    "    all_tweets_tweets = list()\n",
    "    all_tweets_retweets = list()\n",
    "    all_tweets_likes = list()\n",
    "    all_tweets_length = list()\n",
    "    all_tweets_source = list()\n",
    "\n",
    "    #cursor to scrape data from twitter\n",
    "    for tweet in tweepy.Cursor(api.search,\n",
    "                                q= search,\n",
    "                                count = 100 ,\n",
    "                                result_typetweets=\"recent\",\n",
    "                                include_entities=True,\n",
    "                                lang=\"en\" ,\n",
    "                                tweet_mode=\"extended\").items(max_tweets):\n",
    "\n",
    "        all_tweets_timestamps.append(tweet.created_at)\n",
    "        all_tweets_tweets.append(tweet.full_text)\n",
    "        all_tweets_retweets.append(tweet.retweet_count)\n",
    "        all_tweets_likes.append(tweet.favorite_count)\n",
    "        all_tweets_length.append(len(tweet.full_text))\n",
    "        all_tweets_source.append(tweet.source)\n",
    "\n",
    "        #only load tweet to arrays if the NOT retweet\n",
    "        if(tweet.full_text[0:2] != \"RT\"):\n",
    "\n",
    "            #analyze the sentiment\n",
    "            temp = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet.full_text).split())\n",
    "            analysis = TextBlob(temp)\n",
    "            if analysis.sentiment.polarity < 0:\n",
    "                num = -1\n",
    "            elif analysis.sentiment.polarity > 0:\n",
    "                num = 1\n",
    "            else:\n",
    "                num = 0\n",
    "\n",
    "            #add sentiment to array\n",
    "            tweets_sentiment.append(num)\n",
    "            tweets_raw_sentiment.append(analysis.sentiment.polarity)\n",
    "\n",
    "            #add all attributes of tweet to arrays\n",
    "            timestamps.append(tweet.created_at)\n",
    "            tweets.append(tweet.full_text)\n",
    "            tweets_retweets.append(tweet.retweet_count)\n",
    "            tweets_likes.append(tweet.favorite_count)\n",
    "            tweets_length.append(len(tweet.full_text))\n",
    "            tweets_source.append(tweet.source)\n",
    "\n",
    "    #message when done\n",
    "    print(\"done querying.\")\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    datestr = now.strftime('%Y-%m-%d')\n",
    "    time = now.strftime('%H-%M')\n",
    "\n",
    "\n",
    "    #check if the directory exist\n",
    "    #if it doesn't create it\n",
    "    directory = \"data/\" + datestr + \"/\"\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    directory = \"data/\" + datestr + \"/\" + search + \"/\"\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    #create file name for excel file and create wrttier to write to excel\n",
    "    xls_file = directory + search + \"_\" + time + \"_data.xlsx\"\n",
    "    writer = pd.ExcelWriter(xls_file)\n",
    "\n",
    "    ### NON RT TWEETS ###\n",
    "\n",
    "    #use pandas package to export data to csv file\n",
    "    #prepare dataframe to export only on rt tweets to csv\n",
    "    df = pd.DataFrame({'timestamp':timestamps, \n",
    "                       'likes': tweets_likes , \n",
    "                       'retweets': tweets_retweets , \n",
    "                       \"source\" : tweets_source ,\n",
    "                       \"length\" : tweets_length ,\n",
    "                       \"sentiment\" : tweets_sentiment ,\n",
    "                       \"raw-sentiment\" : tweets_raw_sentiment ,\n",
    "                       'tweet':tweets})\n",
    "\n",
    "    #write to excel\n",
    "    df.to_excel(writer , sheet_name='non-rt' ,  encoding='utf-8')\n",
    "\n",
    "    ### NON RT TWEETS ###\n",
    "\n",
    "\n",
    "    ### ALL TWEETS ###\n",
    "\n",
    "    #prepare a dataframe to export to csv of all tweets\n",
    "    df = pd.DataFrame({'timestamp':all_tweets_timestamps , \n",
    "                       'likes': all_tweets_likes , \n",
    "                       'retweets': all_tweets_retweets , \n",
    "                       \"source\" : all_tweets_source ,\n",
    "                       \"length\" : all_tweets_length ,\n",
    "                       'tweet':all_tweets_tweets})\n",
    "\n",
    "    #add to excel\n",
    "    df.to_excel(writer , sheet_name='rt' ,  encoding='utf-8')\n",
    "\n",
    "    ### ALL TWEETS ###\n",
    "\n",
    "\n",
    "    ### TWEET SOURCES ###\n",
    "\n",
    "    #create a list of tweet sources from all tweets and write to excel\n",
    "    all_sources = np.array(all_tweets_source)\n",
    "    unique, counts = np.unique(all_sources, return_counts=True)\n",
    "\n",
    "    temp_sum = counts.sum()\n",
    "    all_source_df = pd.DataFrame({\"source\" : unique , \"count\" : counts , \"percentage\" : ((counts / temp_sum) * 100)})\n",
    "    all_source_df = all_source_df.sort_values(by=['count'] , ascending=False)\n",
    "\n",
    "    all_source_df.to_excel(writer , sheet_name='source all tweets' ,  encoding='utf-8')\n",
    "\n",
    "    #create a list of tweet sources from non rt tweets and write to excel\n",
    "    sources = np.array(tweets_source)\n",
    "    unique, counts = np.unique(sources, return_counts=True)\n",
    "\n",
    "    temp_sum = counts.sum()\n",
    "    source_df = pd.DataFrame({\"source\" : unique , \"count\" : counts , \"percentage\" : ((counts / temp_sum) * 100)})\n",
    "    source_df = source_df.sort_values(by=['count'] , ascending=False)\n",
    "\n",
    "    source_df.to_excel(writer , sheet_name='source nonRT tweets' ,  encoding='utf-8')\n",
    "\n",
    "    ### TWEET SOURCES ###\n",
    "\n",
    "    ###  BAG OF WORDS ####\n",
    "\n",
    "    #instace to tokenize words and remove punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    #create a list of english stopwords\n",
    "    #add words with no info used in twitter\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stop_words.add(\"http\")\n",
    "    stop_words.add(\"https\")\n",
    "    stop_words.add(\"co\")\n",
    "    stop_words.add(\"amp\")\n",
    "\n",
    "    #list to collect a count of words\n",
    "    filtered_word_list = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        word_tokens = tokenizer.tokenize(tweet.lower())\n",
    "        for w in word_tokens:\n",
    "            if (not w in stop_words):\n",
    "                filtered_word_list.append(w)\n",
    "\n",
    "    unique, counts = np.unique(filtered_word_list, return_counts=True)\n",
    "    bag_of_words = pd.DataFrame({\"words\" : unique , \"count\" : counts})\n",
    "    bag_of_words = bag_of_words.sort_values(by=['count'] , ascending=False)\n",
    "\n",
    "    #add to excel\n",
    "    bag_of_words.to_excel(writer , sheet_name='most common words' ,  encoding='utf-8')\n",
    "\n",
    "    ###  BAG OF WORDS ####\n",
    "\n",
    "    ### METADATA ###\n",
    "\n",
    "    #get the metadat of the tweets\n",
    "    unique, counts = np.unique(tweets_sentiment , return_counts=True)\n",
    "\n",
    "    #create metadata and output to csv file\n",
    "    meta_df = pd.DataFrame({\"all_retweets\" : [np.mean(all_tweets_retweets)] , \n",
    "                            \"all_likes\" : [np.mean(all_tweets_likes)] , \n",
    "                            \"all_length\" : [np.mean(all_tweets_length)] ,\n",
    "                            \"retweets\" : [np.mean(tweets_retweets)] , \n",
    "                            \"likes\" : [np.mean(tweets_likes)] ,\n",
    "                            \"length\" : [np.mean(tweets_length)] ,\n",
    "                            \"sentiment\" : [np.mean(tweets_sentiment)] , \n",
    "                            \"sentiment-\" + str(unique[0]) : counts[0] ,\n",
    "                            \"sentiment-\" + str(unique[1]) : counts[1] ,\n",
    "                            \"sentiment-\" + str(unique[2]) : counts[2] ,\n",
    "                            \"chi-squared_test\" :  chisquare(counts).statistic , \n",
    "                            \"chisquare-p-val\" : chisquare(counts).pvalue ,\n",
    "                            \"tweet-rt-ratio\" : [len(tweets_retweets) / len(all_tweets_retweets)]})\n",
    "\n",
    "    #add to excel\n",
    "    meta_df.to_excel(writer , sheet_name='metadata' ,  encoding='utf-8')\n",
    "\n",
    "    ### METADATA ###\n",
    "\n",
    "    ### COMPARE TO PAST TWEETS ###\n",
    "\n",
    "    tweet_analysis_df = pd.read_excel(\"tweet-analysis-summary.xlsx\" , sheet_name=\"data\")\n",
    "\n",
    "    topics = tweet_analysis_df[\"topic\"]\n",
    "\n",
    "    if search in set(topics):\n",
    "        temp = tweet_analysis_df.loc[tweet_analysis_df['topic'] == search]\n",
    "\n",
    "        old_neg = temp[-1].values[0]\n",
    "        old_neu = temp[0].values[0]\n",
    "        old_pos = temp[1].values[0]\n",
    "        \n",
    "        temp[-1] +=  counts[0]\n",
    "        temp[0] += counts[1]\n",
    "        temp[1] += counts[2]\n",
    "        \n",
    "        obs = [counts[0] , counts[1] , counts[2]]\n",
    "        exp = [old_neg , old_neu , old_pos]\n",
    "        \n",
    "        try:\n",
    "            test_stat , pval = chisquare( f_obs=obs , f_exp=exp )\n",
    "        except:\n",
    "            test_stat , pval = \"err\" , \"err\"\n",
    "\n",
    "        out_df = pd.DataFrame({ -1 : [counts[0] , old_neg] , \n",
    "                                 0 : [counts[1] , old_neu],\n",
    "                                 1 : [counts[2] , old_pos] , \n",
    "                                \"values\" : [ test_stat , pval ] , \n",
    "                                \"labels\" : [ \"chi-sq-val\" , \"pvalue\"]})\n",
    "\n",
    "\n",
    "        tweet_analysis_df.loc[tweet_analysis_df['topic'] == search] = temp\n",
    "\n",
    "    else:\n",
    "        temp = [search , counts[0] , counts[1] , counts[2] ]\n",
    "\n",
    "        tweet_analysis_df.loc[len(tweet_analysis_df)] = temp\n",
    "\n",
    "        sum = counts[0] + counts[1] + counts[2]\n",
    "        avg = sum/3\n",
    "        \n",
    "        obs = [counts[0] , counts[1] , counts[2]]\n",
    "        exp = [avg , avg ,avg]\n",
    "\n",
    "        try:\n",
    "            test_stat , pval = chisquare( f_obs=obs , f_exp=exp )\n",
    "        except:\n",
    "            test_stat , pval = \"err\" , \"err\"\n",
    "\n",
    "        out_df = pd.DataFrame({ -1 : [counts[0] , avg] , \n",
    "                                 0 : [counts[1] , avg],\n",
    "                                 1 : [counts[2] , avg] , \n",
    "                                \"values\" : [ test_stat , pval ] , \n",
    "                                \"labels\" : [ \"chi-sq-val\" , \"pvalue\"]})\n",
    "\n",
    "    tweet_analysis_df.to_excel(\"tweet-analysis-summary.xlsx\" , sheet_name=\"data\")\n",
    "\n",
    "\n",
    "    #add to excel\n",
    "    out_df.to_excel(writer , sheet_name='compare-to-old' ,  encoding='utf-8')\n",
    "\n",
    "    ### COMPARE TO PAST TWEETS ###\n",
    "\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "    #output to user\n",
    "    print(\"full file created: \" , xls_file)\n",
    "    \n",
    "query_twitter(\"caravan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
