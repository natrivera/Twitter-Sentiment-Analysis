{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in\n"
     ]
    }
   ],
   "source": [
    "#tweepy package to coolect tweets\n",
    "import tweepy\n",
    "import twitter_keys\n",
    "\n",
    "#authentication\n",
    "auth = tweepy.OAuthHandler(twitter_keys.CONSUMER_KEY, twitter_keys.CONSUMER_SECRET)\n",
    "auth.set_access_token(twitter_keys.ACCESS_KEY, twitter_keys.ACCESS_SECRET)\n",
    "\n",
    "#create instance of api\n",
    "api = tweepy.API(auth , wait_on_rate_limit=True)\n",
    "print(\"logged in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#ENTER SEARCH TERM HERE\n",
    "search = \"semester\"\n",
    "\n",
    "#enter the max number of tweets\n",
    "max_tweets = 3000\n",
    "\n",
    "#arrays to collect data of only non rt tweets\n",
    "timestamps = list()\n",
    "tweets = list()\n",
    "tweets_retweets = list()\n",
    "tweets_likes = list()\n",
    "tweets_length = list()\n",
    "tweets_source = list()\n",
    "tweets_sentiment = list()\n",
    "\n",
    "#arrays to collect data of all tweets\n",
    "all_tweets_timestamps = list();\n",
    "all_tweets_tweets = list()\n",
    "all_tweets_retweets = list()\n",
    "all_tweets_likes = list()\n",
    "all_tweets_length = list()\n",
    "all_tweets_source = list()\n",
    "\n",
    "#cursor to scrape data from twitter\n",
    "for tweet in tweepy.Cursor(api.search,\n",
    "                            q= search,\n",
    "                            count = 100 ,\n",
    "                            result_typetweets=\"recent\",\n",
    "                            include_entities=True,\n",
    "                            lang=\"en\" ,\n",
    "                            tweet_mode=\"extended\").items(max_tweets):\n",
    "    \n",
    "    all_tweets_timestamps.append(tweet.created_at)\n",
    "    all_tweets_tweets.append(tweet.full_text)\n",
    "    all_tweets_retweets.append(tweet.retweet_count)\n",
    "    all_tweets_likes.append(tweet.favorite_count)\n",
    "    all_tweets_length.append(len(tweet.full_text))\n",
    "    all_tweets_source.append(tweet.source)\n",
    "\n",
    "    #only load tweet to arrays if the NOT retweet\n",
    "    if(tweet.full_text[0:2] != \"RT\"):\n",
    "        \n",
    "        #analyze the sentiment\n",
    "        temp = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet.full_text).split())\n",
    "        analysis = TextBlob(temp)\n",
    "        if analysis.sentiment.polarity < 0:\n",
    "            num = -1\n",
    "        elif analysis.sentiment.polarity > 0:\n",
    "            num = 1\n",
    "        else:\n",
    "            num = 0\n",
    "         \n",
    "        #add sentiment to array\n",
    "        tweets_sentiment.append(num)\n",
    "        \n",
    "        #add all attributes of tweet to arrays\n",
    "        timestamps.append(tweet.created_at)\n",
    "        tweets.append(tweet.full_text)\n",
    "        tweets_retweets.append(tweet.retweet_count)\n",
    "        tweets_likes.append(tweet.favorite_count)\n",
    "        tweets_length.append(len(tweet.full_text))\n",
    "        tweets_source.append(tweet.source)\n",
    "\n",
    "#message when done\n",
    "print(\"done querying.\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "datestr = now.strftime('%Y-%m-%d')\n",
    "time = now.strftime('%H-%M')\n",
    "\n",
    " \n",
    "#check if the directory exist\n",
    "#if it doesn't create it\n",
    "directory = \"data/\" + datestr + \"/\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "directory = \"data/\" + datestr + \"/\" + search + \"/\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "#create file name for excel file and create wrttier to write to excel\n",
    "xls_file = directory + search + \"_\" + time + \"_data.xlsx\"\n",
    "writer = pd.ExcelWriter(xls_file)\n",
    "\n",
    "#use pandas package to export data to csv file\n",
    "#prepare dataframe to export only on rt tweets to csv\n",
    "df = pd.DataFrame({'timestamp':timestamps, \n",
    "                   'likes': tweets_likes , \n",
    "                   'retweets': tweets_retweets , \n",
    "                   \"source\" : tweets_source ,\n",
    "                   \"length\" : tweets_length ,\n",
    "                   \"sentiment\" : tweets_sentiment ,\n",
    "                   'tweet':tweets})\n",
    "\n",
    "#write to excel\n",
    "df.to_excel(writer , sheet_name='non-rt' ,  encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "#prepare a dataframe to export to csv of all tweets\n",
    "df = pd.DataFrame({'timestamp':all_tweets_timestamps , \n",
    "                   'likes': all_tweets_likes , \n",
    "                   'retweets': all_tweets_retweets , \n",
    "                   \"source\" : all_tweets_source ,\n",
    "                   \"length\" : all_tweets_length ,\n",
    "                   'tweet':all_tweets_tweets})\n",
    "\n",
    "#create csv\n",
    "df.to_excel(writer , sheet_name='rt' ,  encoding='utf-8')\n",
    "\n",
    "\n",
    "#create metadata and output to csv file\n",
    "meta_df = pd.DataFrame({\"all_retweets\" : [np.mean(all_tweets_retweets)] , \n",
    "                        \"all_likes\" : [np.mean(all_tweets_likes)] , \n",
    "                        \"all_length\" : [np.mean(all_tweets_length)] ,\n",
    "                        \"retweets\" : [np.mean(tweets_retweets)] , \n",
    "                        \"likes\" : [np.mean(tweets_likes)] ,\n",
    "                        \"length\" : [np.mean(tweets_length)] ,\n",
    "                        \"sentiment\" : [np.mean(tweets_sentiment)] , \n",
    "                        \"tweet-rt-ratio\" : [len(tweets_retweets) / len(all_tweets_retweets)]})\n",
    "\n",
    "\n",
    "#create csv\n",
    "meta_df.to_excel(writer , sheet_name='metadata' ,  encoding='utf-8')\n",
    "\n",
    "#create a list of tweet sources from all tweets and write to excel\n",
    "all_sources = np.array(all_tweets_source)\n",
    "unique, counts = np.unique(all_sources, return_counts=True)\n",
    "\n",
    "all_source_df = pd.DataFrame({\"source\" : unique , \"count\" : counts , \"percentage\" : (counts / sum(counts)) * 100})\n",
    "all_source_df = all_source_df.sort_values(by=['count'] , ascending=False)\n",
    "\n",
    "all_source_df.to_excel(writer , sheet_name='source all tweets' ,  encoding='utf-8')\n",
    "\n",
    "#create a list of tweet sources from non rt tweets and write to excel\n",
    "sources = np.array(tweets_source)\n",
    "unique, counts = np.unique(sources, return_counts=True)\n",
    "\n",
    "source_df = pd.DataFrame({\"source\" : unique , \"count\" : counts , \"percentage\" : (counts / sum(counts)) * 100})\n",
    "source_df = source_df.sort_values(by=['count'] , ascending=False)\n",
    "\n",
    "source_df.to_excel(writer , sheet_name='source nonRT tweets' ,  encoding='utf-8')\n",
    "\n",
    "### CREATE A BAG OF WORDS FOR EACH SET ####\n",
    "\n",
    "#instace to tokenize words and remove punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#create a list of english stopwords\n",
    "#add words with no info used in twitter\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.add(\"http\")\n",
    "stop_words.add(\"https\")\n",
    "stop_words.add(\"co\")\n",
    "stop_words.add(\"amp\")\n",
    "\n",
    "#list to collect a count of words\n",
    "filtered_word_list = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    word_tokens = tokenizer.tokenize(tweet.lower())\n",
    "    for w in word_tokens:\n",
    "        if (not w in stop_words):\n",
    "            filtered_word_list.append(w)\n",
    "            \n",
    "unique, counts = np.unique(filtered_word_list, return_counts=True)\n",
    "bag_of_words = pd.DataFrame({\"words\" : unique , \"count\" : counts})\n",
    "bag_of_words = bag_of_words.sort_values(by=['count'] , ascending=False)\n",
    "\n",
    "bag_of_words.to_excel(writer , sheet_name='most common words' ,  encoding='utf-8')\n",
    "\n",
    "writer.save()\n",
    "\n",
    "#output to user\n",
    "print(\"full file created: \" , xls_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
